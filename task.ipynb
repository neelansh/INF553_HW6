{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import math\n",
    "from collections import Counter, defaultdict\n",
    "import itertools\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './hw6_clustering.txt'\n",
    "seed = 1\n",
    "random.seed(seed)\n",
    "\n",
    "def data_generator():\n",
    "    data = np.loadtxt(file_path, delimiter=',')\n",
    "    data = data.tolist()\n",
    "    random.shuffle(data)\n",
    "    data_batches = []\n",
    "    for i in range(0, len(data), math.floor(0.2*len(data))):\n",
    "        d = np.array(data[i:i+math.floor(0.2*len(data))])\n",
    "        data_batches.append(d)\n",
    "        \n",
    "    return data_batches\n",
    "\n",
    "data_batches = data_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class stats:\n",
    "    def __init__(self, points):\n",
    "        # create the stats from points\n",
    "        self.point_indices = points[:, 0].tolist()\n",
    "        self.actual_clusters = points[:, 1].tolist()\n",
    "        self.n = len(points)\n",
    "        self.sum = np.sum(points[:, 2:], axis=0)\n",
    "        self.sumsq = np.sum(np.power(points[:, 2:], 2), axis=0)\\\n",
    "        \n",
    "    def update_stats(self, point):\n",
    "        self.point_indices.append(point[0])\n",
    "        self.actual_clusters.append(point[1])\n",
    "        self.n += 1\n",
    "        self.sum += point[2:]\n",
    "        self.sumsq += np.power(point[2:], 2)\n",
    "        \n",
    "        return \n",
    "    \n",
    "    def merge_with(self, cluster):\n",
    "        self.point_indices += cluster.point_indices\n",
    "        self.actual_clusters += cluster.actual_clusters\n",
    "        self.n += cluster.n\n",
    "        self.sum += cluster.sum\n",
    "        self.sumsq += cluster.sumsq\n",
    "        \n",
    "    def calculate_variance(self):\n",
    "        return np.power((self.sumsq/self.n) - np.power(self.sum/self.n, 2), 1/2)\n",
    "    \n",
    "    def calculate_centroid(self):\n",
    "        return self.sum / self.n\n",
    "    \n",
    "def mahalanobis_distance(cluster, point):\n",
    "    return np.power(np.sum(np.power((point - cluster.calculate_centroid()) / cluster.calculate_variance(), 2)), 1/2)\n",
    "\n",
    "\n",
    "\n",
    "def bfr(data, k, d, output_file_path):\n",
    "    global seed\n",
    "    rs = defaultdict(list)\n",
    "    ds = defaultdict(list)\n",
    "    cs = defaultdict(list)\n",
    "    \n",
    "    output_file = open(output_file_path, \"wt\")\n",
    "    output_file.write(\"The intermediate results:\\n\")\n",
    "    for i, batch in enumerate(data):\n",
    "        if(i == 0):\n",
    "            #  Run K-Means (e.g., from sklearn) with a large K (e.g., 5 times of the number of the input clusters) on the data in memory using the Euclidean distance as the similarity measurement.\n",
    "            model = KMeans(n_clusters=k*5, random_state=seed)\n",
    "            model = model.fit(batch[:, 2:])\n",
    "            \n",
    "            # In the K-Means result from Step 2, move all the clusters that contain only one point to RS\n",
    "            index = defaultdict(list)\n",
    "            for pos, centroid_id in enumerate(model.labels_):\n",
    "                index[centroid_id].append(pos)\n",
    "                \n",
    "            rest_of_data = []\n",
    "            \n",
    "            for centroid_id, positions in index.items():\n",
    "                if(len(positions) == 1):\n",
    "                    rs[centroid_id] = np.take(batch, positions, axis=0)\n",
    "                if(len(positions) > 1):\n",
    "                    rest_of_data.append(np.take(batch, positions, axis=0))\n",
    "            \n",
    "            rest_of_data = np.concatenate(rest_of_data, axis=0)\n",
    "            # Run K-Means again to cluster the rest of the data points with K = the number of input clusters.\n",
    "            model = KMeans(n_clusters=k, random_state=seed)\n",
    "            model = model.fit(rest_of_data[:, 2:])\n",
    "\n",
    "            # Use the K-Means result from Step 4 to generate the DS clusters (i.e., discard their points and generate statistics).\n",
    "            index = defaultdict(list)\n",
    "            for pos, centroid_id in enumerate(model.labels_):\n",
    "                index[centroid_id].append(pos)\n",
    "            \n",
    "            for centroid_id, positions in index.items():\n",
    "                points = np.take(rest_of_data, positions, axis=0)\n",
    "                ds[centroid_id] = stats(points)\n",
    "                \n",
    "            points = [x for x in rs.values()]\n",
    "            points = np.concatenate(points, axis=0)\n",
    "            \n",
    "            model = KMeans(n_clusters=min(5*k, len(points)), random_state=seed)\n",
    "            model = model.fit(points[:, 2:])\n",
    "            \n",
    "            index = defaultdict(list)\n",
    "            for pos, centroid_id in enumerate(model.labels_):\n",
    "                index[centroid_id].append(pos)\n",
    "            \n",
    "            rs = defaultdict(list)\n",
    "            for centroid_id, positions in index.items():\n",
    "                p = np.take(points, positions, axis=0)\n",
    "                if(len(positions) == 1):\n",
    "                    rs[centroid_id] = p\n",
    "                if(len(positions) > 1):\n",
    "                    cs[centroid_id] = stats(p)\n",
    "            \n",
    "        else:\n",
    "            for point in batch:\n",
    "                cluster_id, min_dist = min(list(map(lambda x: (x[0], mahalanobis_distance(x[1], point[2:])), ds.items())), key=lambda x: x[1])\n",
    "                \n",
    "                if(min_dist < 2*math.pow(d, 1/2)):\n",
    "                    ds[cluster_id].update_stats(point)\n",
    "                else:\n",
    "                    if(len(cs) != 0):\n",
    "                        cluster_id, min_dist = min(list(map(lambda x: (x[0], mahalanobis_distance(x[1], point[2:])), cs.items())), key=lambda x: x[1])\n",
    "                        if(min_dist < 2*math.pow(d, 1/2)):\n",
    "                            cs[cluster_id].update_stats(point)\n",
    "                        else:\n",
    "                            if(len(rs) == 0):\n",
    "                                rs[0] = np.expand_dims(point, axis=0)\n",
    "                            else:\n",
    "                                rs[max(rs.keys())+1] = np.expand_dims(point, axis=0)\n",
    "                    else:\n",
    "                        if(len(rs) == 0):\n",
    "                            rs[0] = np.expand_dims(point, axis=0)\n",
    "                        else:\n",
    "                            rs[max(rs.keys())+1] = np.expand_dims(point, axis=0)\n",
    "                        \n",
    "            points = [x for x in rs.values()]\n",
    "            points = np.concatenate(points, axis=0)\n",
    "            \n",
    "            model = KMeans(n_clusters=min(5*k, len(points)), random_state=seed)\n",
    "            model = model.fit(points[:, 2:])\n",
    "            \n",
    "            index = defaultdict(list)\n",
    "            for pos, centroid_id in enumerate(model.labels_):\n",
    "                index[centroid_id].append(pos)\n",
    "            \n",
    "            rs = defaultdict(list)\n",
    "            for centroid_id, positions in index.items():\n",
    "                p = np.take(points, positions, axis=0)\n",
    "                if(len(positions) == 1):\n",
    "                    rs[centroid_id] = p\n",
    "                if(len(positions) > 1):\n",
    "                    if(len(cs) == 0):\n",
    "                        cs[0] = stats(p)\n",
    "                    else:\n",
    "                        cs[max(cs.keys())+1] = stats(p)\n",
    "                    \n",
    "            # merge cs clusters if distance < 2 root d\n",
    "            to_be_merged = []\n",
    "            for c1, c2 in itertools.combinations(cs.keys(), 2):\n",
    "                dist = mahalanobis_distance(cs[c1], cs[c2].calculate_centroid())\n",
    "                if(dist < 2*math.pow(dist, 1/2)):\n",
    "                    to_be_merged.append((c1, c2))\n",
    "                    \n",
    "            for (c1, c2) in to_be_merged:\n",
    "                if(c1 in cs and c2 in cs):\n",
    "                    cs[c1].merge_with(cs[c2])\n",
    "                    del cs[c2]\n",
    "        # after each round output\n",
    "        number_of_ds_points = sum([x.n for x in ds.values()])\n",
    "        number_of_clusters_cs = len(cs)\n",
    "        number_of_cs_points = sum([x.n for x in cs.values()])\n",
    "        number_of_rs_points = sum([len(x) for x in rs.values()])\n",
    "        if(i != len(data)-1):\n",
    "            output_file.write(\"Round {}: {},{},{},{}\\n\".format(i+1, number_of_ds_points, number_of_clusters_cs, number_of_cs_points, number_of_rs_points))\n",
    "        \n",
    "        \n",
    "    # after last round\n",
    "    \n",
    "    # merge cs with ds with distance less than 2 root d\n",
    "    merged_cs = []\n",
    "    for k, c in cs.items():\n",
    "        point = c.calculate_centroid()\n",
    "        cluster_id, min_dist = min(list(map(lambda x: (x[0], mahalanobis_distance(x[1], point)), ds.items())), key=lambda x: x[1])\n",
    "        if(min_dist < 2*math.pow(d, 1/2)):\n",
    "            ds[cluster_id].merge_with(c)\n",
    "            merged_cs.append(k)\n",
    "\n",
    "    for k in merged_cs:\n",
    "        del cs[k]\n",
    "        \n",
    "    number_of_ds_points = sum([x.n for x in ds.values()])\n",
    "    number_of_clusters_cs = len(cs)\n",
    "    number_of_cs_points = sum([x.n for x in cs.values()])\n",
    "    number_of_rs_points = sum([len(x) for x in rs.values()])\n",
    "    output_file.write(\"Round {}: {},{},{},{}\\n\".format(len(data), number_of_ds_points, number_of_clusters_cs, number_of_cs_points, number_of_rs_points))\n",
    "        \n",
    "    gt = []\n",
    "    pred = []\n",
    "    original_index = []\n",
    "    cluster_id = 0\n",
    "    for i, x in ds.items():\n",
    "        gt += x.actual_clusters\n",
    "        pred += [cluster_id]*x.n\n",
    "        original_index += x.point_indices\n",
    "        cluster_id += 1\n",
    "\n",
    "\n",
    "    for i, x in cs.items():\n",
    "        gt += x.actual_clusters\n",
    "        pred += [cluster_id]*x.n\n",
    "        original_index += x.point_indices\n",
    "        cluster_id += 1\n",
    "\n",
    "    for i, x in rs.items():\n",
    "        gt += x[:, 1].tolist()\n",
    "        original_index += x[:, 0].tolist()\n",
    "        pred += [-1]*len(x)\n",
    "    \n",
    "    gt = [int(x) for x in gt]\n",
    "    pred = [int(x) for x in pred]\n",
    "    original_index = [int(x) for x in original_index]\n",
    "    final_output = sorted([(x,y) for x,y in zip(original_index, pred)])\n",
    "    \n",
    "    output_file.write(\"The clustering results:\\n\")\n",
    "    for x, y in final_output:\n",
    "        output_file.write(\"{},{}\\n\".format(x, y))\n",
    "    \n",
    "    from sklearn.metrics.cluster import v_measure_score\n",
    "    print(v_measure_score(gt, pred))\n",
    "    \n",
    "    output_file.close()\n",
    "    return ds, cs, rs \n",
    "            \n",
    "        \n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.999763285767253\n",
      "70.77719020843506\n"
     ]
    }
   ],
   "source": [
    "st = time.time()\n",
    "d = data_batches[0].shape[-1]-2\n",
    "ds, cs, rs = bfr(data_batches, 10, d, \"output.txt\")\n",
    "print(time.time()-st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import v_measure_score\n",
    "v_measure_score(gt, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9.98388151,  9.95067318,  9.96774954, 10.04369787, 10.15728946,\n",
       "        9.91627251,  9.93356764, 10.02789976,  9.93499773,  9.96334103])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[6].calculate_variance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.545697308638782"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mahalanobis_distance(ds[4], data_batches[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[182585.        ,      0.        ,    186.94646621,\n",
       "          -197.54626052,   -187.61672574,    205.60521327,\n",
       "           207.95931784,   -190.98863908,    212.07898855,\n",
       "           209.88670571,    201.3014873 ,   -203.58424924]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.expand_dims(data_batches[0][0], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {1:1, 2: 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "del d[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: 2}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters=k*5, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.fit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = {}\n",
    "for i, v in enumerate(model.labels_):\n",
    "    if(v in clusters):\n",
    "        clusters[v].append(i)\n",
    "    else:\n",
    "        clusters[v] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = []\n",
    "for i, v in clusters.items():\n",
    "    if(len(v) == 1):\n",
    "        mask.append(True)\n",
    "    else:\n",
    "        mask += [False] * len(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Counter(model.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_rs = set()\n",
    "for i, v in c.items():\n",
    "    if(v == 1):\n",
    "        set_rs.add(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{23, 30, 35, 38}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([17, 17, 29, ..., 36, 48, 21], dtype=int32)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  488.32078147,   806.6889544 ,   504.31129676,   750.32024191,\n",
       "          668.42300732,   602.8089268 ,   467.42711802,   604.3628392 ,\n",
       "          709.5253836 ,   837.45834081],\n",
       "       [ -657.65016942,  -937.97207849,  -594.59746982,  -726.13482071,\n",
       "         -904.71434022,  -933.76791914,  -681.39171522,  -879.1373381 ,\n",
       "        -1114.5440414 ,  -703.31236384],\n",
       "       [ -594.98142501,  -985.41613544,  -604.4450682 ,  -882.78458907,\n",
       "         -672.02636454,  -538.63116134,  -562.0532969 ,  -548.75358638,\n",
       "         -734.67639734,  -657.57533124],\n",
       "       [  767.41409789,   597.16650011,  1027.15865272,   783.46886552,\n",
       "         1138.26132748,   663.44645596,   903.03076441,   712.9069162 ,\n",
       "          985.9498533 ,   906.75844133]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[list(map(lambda x: x in set_rs, model.labels_))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_index = list(map(lambda v: len(v[1])==1, clusters.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "boolean index did not match indexed array along dimension 0; dimension is 64462 but corresponding boolean dimension is 50",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-984af23876f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrs_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: boolean index did not match indexed array along dimension 0; dimension is 64462 but corresponding boolean dimension is 50"
     ]
    }
   ],
   "source": [
    "x[rs_index, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
